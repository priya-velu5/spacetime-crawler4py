{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many unique pages did you find? Uniqueness is established by the URL, but discarding the fragment part. \n",
    "So, for example, http://www.ics.uci.edu#aaa and http://www.ics.uci.edu#bbb are the same URL.\n",
    "\n",
    "2. What is the longest page in terms of number of words? (HTML markup doesnâ€™t count as words)\n",
    "3. What are the 50 most common words in the entire set of pages? (Ignore English stop words, which can be found, for example, hereLinks to an external site.) Submit the list of common words ordered by frequency.\n",
    "4. How many subdomains did you find in the ics.uci.edu domain? Submit the list of subdomains ordered \n",
    "alphabetically and the number of unique pages detected in each subdomain. \n",
    "The content of this list should be lines containing URL, number, for example:\n",
    "http://vision.ics.uci.edu, 10 (not the actual number here)\n",
    "\n",
    "*.ics.uci.edu/*\n",
    "*.cs.uci.edu/*\n",
    "*.informatics.uci.edu/*\n",
    "*.stat.uci.edu/*\n",
    "today.uci.edu/department/information_computer_sciences/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read the downloaded files as a beautiful soup object\n",
    "\n",
    "def extract_content(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/singaram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading wordtokenize: Package 'wordtokenize' not\n",
      "[nltk_data]     found in index\n",
      "[nltk_data] Downloading package punkt to /home/singaram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130\n",
      "sli.ics.uci.edu_PmWiki_TextFormattingRules#DefinitionLists.txt\n"
     ]
    }
   ],
   "source": [
    "def get_file_names(folder_path):\n",
    "    return [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# Usage\n",
    "folder_path = 'results10/downloaded/'\n",
    "downloaded_files = get_file_names(folder_path)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordtokenize')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "max_doc_length = 0\n",
    "max_file = ''\n",
    "tokens = []\n",
    "for file in random.sample(downloaded_files,500):\n",
    "    soup = extract_content(folder_path+file)\n",
    "    text = soup.get_text()\n",
    "    #print(text)\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    l = 0\n",
    "    for line in lines:\n",
    "        if len(line) > 0:\n",
    "            #print(line)\n",
    "            l+=len(line.split(' '))\n",
    "            #token_temp = re.findall(r'\\b[a-zA-Z]+\\b', line.lower()) #word_tokenize(line) #re.findall('[a-zA-Z]', line.lower()) #word_tokenize(line) re.findall('[a-zA-Z]', line.lower())\n",
    "            #token_temp= [w for w in token_temp if not w.lower() in stop_words]\n",
    "            #tokens = np.append(tokens,token_temp)\n",
    "            word = re.findall(r'\\b([a-zA-Z]{2,})\\b', line.lower())\n",
    "            tokens.extend(word)\n",
    "            \n",
    "    if l > max_doc_length:\n",
    "        max_doc_length = l\n",
    "        max_file = file\n",
    "    \n",
    "    tokens_cleaned = [w for w in tokens if not w.lower() in stop_words]\n",
    "    tokens = tokens_cleaned\n",
    "    \n",
    "\n",
    "print(max_doc_length)\n",
    "print(max_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def computeWordFrequencies(token_list):\n",
    "    # Count the occurrences of each token\n",
    "    # Counter creates a dictionary of the words and frequencies\n",
    "    \"\"\"\n",
    "    This method runs linear to the input. Making a counter variable has O(n) time complexity. \n",
    "    reference: https://stackoverflow.com/questions/42461840/what-is-the-time-complexity-of-collections-counter-in-python\n",
    "\n",
    "    \"\"\"\n",
    "    word_frequencies = Counter(token_list)\n",
    "    return word_frequencies\n",
    "\n",
    "\n",
    "def printWordFrequencies(word_freq):\n",
    "    \"\"\"\n",
    "    This function runs in O(nlogn) time complexity as thats the time taken to sort the Counter data structure\n",
    "    by its frequency. \n",
    "\n",
    "    Reference: https://stackoverflow.com/questions/29240807/python-collections-counter-most-common-complexity\n",
    "\n",
    "    \"\"\"\n",
    "    # Print word frequencies in decreasing order\n",
    "    for token, count in word_freq.most_common(50):\n",
    "        print(f\"{token}: {count}\")\n",
    "    \n",
    "word_freq = computeWordFrequencies(tokens)\n",
    "# remove stop words from the word_freq\n",
    "\n",
    "printWordFrequencies(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "1. How many unique pages did you find? Uniqueness is established by the URL,\n",
    "but discarding the fragment part. So, for example,\n",
    "http://www.ics.uci.edu#aaa and http://www.ics.uci.edu#bbb are the same URL.\n",
    "\n",
    "4. How many subdomains did you find in the ics.uci.edu domain?\n",
    "Submit the list of subdomains ordered alphabetically and the number of\n",
    "unique pages detected in each subdomain.\n",
    "\n",
    "'''\n",
    "\n",
    "def subdomainsCount(domain, file):\n",
    "    # Function to extract subdomain from URL\n",
    "    def extract_subdomain(url):\n",
    "        parsed_url = urlparse(url)\n",
    "        if parsed_url.netloc.endswith(domain):\n",
    "            subdomains = parsed_url.netloc.split('.')\n",
    "            if len(subdomains) > 2:\n",
    "                return subdomains[0]\n",
    "        return None\n",
    "\n",
    "    # Count unique pages for subdomains of the specified domain\n",
    "    subdomain_page_counts = defaultdict(int)\n",
    "    with open(file, 'r') as file:\n",
    "        for url in file:\n",
    "            url = url.strip()\n",
    "            subdomain = extract_subdomain(url)\n",
    "            if subdomain and subdomain != 'www':\n",
    "                subdomain_page_counts[subdomain] += 1\n",
    "\n",
    "    # Construct the result dictionary\n",
    "    result = {}\n",
    "    sorted_subdomains = sorted(subdomain_page_counts.items(), key=lambda x: x[0])\n",
    "    for subdomain, count in sorted_subdomains:\n",
    "        result[f\"http://{subdomain}.{domain}\"] = count\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def count_unique_pages(file_path):\n",
    "    unique_pages = set()\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            url = line.strip()\n",
    "            parsed_url = urlparse(url)\n",
    "\n",
    "            # Discard the fragment part for uniqueness\n",
    "            cleaned_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}\"\n",
    "\n",
    "            unique_pages.add(cleaned_url)\n",
    "\n",
    "    return unique_pages, len(unique_pages)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique pages found: 5810\n",
      ".ics.uci.edu\n",
      "{'http://Transformativeplay..ics.uci.edu': 1, 'http://acoi..ics.uci.edu': 73, 'http://aiclub..ics.uci.edu': 1, 'http://asterix..ics.uci.edu': 4, 'http://betapro..ics.uci.edu': 3, 'http://calendar..ics.uci.edu': 1, 'http://cdb..ics.uci.edu': 33, 'http://chenli..ics.uci.edu': 3, 'http://circadiomics..ics.uci.edu': 8, 'http://cml..ics.uci.edu': 5, 'http://code..ics.uci.edu': 13, 'http://computableplant..ics.uci.edu': 45, 'http://cradl..ics.uci.edu': 29, 'http://create..ics.uci.edu': 4, 'http://cwicsocal18..ics.uci.edu': 10, 'http://cybert..ics.uci.edu': 1, 'http://dejavu..ics.uci.edu': 2, 'http://elms..ics.uci.edu': 5, 'http://emj..ics.uci.edu': 8, 'http://evoke..ics.uci.edu': 5, 'http://flamingo..ics.uci.edu': 27, 'http://fr..ics.uci.edu': 9, 'http://frost..ics.uci.edu': 3, 'http://futurehealth..ics.uci.edu': 22, 'http://grape..ics.uci.edu': 1445, 'http://graphics..ics.uci.edu': 17, 'http://graphmod..ics.uci.edu': 3, 'http://hack..ics.uci.edu': 1, 'http://hpi..ics.uci.edu': 5, 'http://ibook..ics.uci.edu': 7, 'http://industryshowcase..ics.uci.edu': 3, 'http://ipubmed..ics.uci.edu': 1, 'http://isg..ics.uci.edu': 124, 'http://jgarcia..ics.uci.edu': 24, 'http://labbie..ics.uci.edu': 1, 'http://luci..ics.uci.edu': 1, 'http://mcs..ics.uci.edu': 11, 'http://mdogucu..ics.uci.edu': 6, 'http://mlphysics..ics.uci.edu': 1, 'http://mondego..ics.uci.edu': 14, 'http://motifmap..ics.uci.edu': 2, 'http://motifmap-rna..ics.uci.edu': 2, 'http://mt-live..ics.uci.edu': 2, 'http://mupro..ics.uci.edu': 2, 'http://nalini..ics.uci.edu': 2, 'http://oai..ics.uci.edu': 2, 'http://old-reactions..ics.uci.edu': 9, 'http://perennialpolycultures..ics.uci.edu': 1, 'http://psearch..ics.uci.edu': 4, 'http://reactions..ics.uci.edu': 7, 'http://redmiles..ics.uci.edu': 5, 'http://riscit..ics.uci.edu': 1, 'http://scratch..ics.uci.edu': 3, 'http://sdcl..ics.uci.edu': 172, 'http://se..ics.uci.edu': 1, 'http://seal..ics.uci.edu': 5, 'http://selectpro..ics.uci.edu': 5, 'http://sli..ics.uci.edu': 683, 'http://sourcerer..ics.uci.edu': 1, 'http://stairs..ics.uci.edu': 4, 'http://student-council..ics.uci.edu': 26, 'http://studentcouncil..ics.uci.edu': 21, 'http://students..ics.uci.edu': 1, 'http://summeracademy..ics.uci.edu': 6, 'http://swiki..ics.uci.edu': 576, 'http://tad..ics.uci.edu': 3, 'http://transformativeplay..ics.uci.edu': 6, 'http://tutors..ics.uci.edu': 44, 'http://ugradforms..ics.uci.edu': 1, 'http://wics..ics.uci.edu': 1039}\n",
      ".cs.uci.edu\n",
      "{}\n",
      ".informatics.uci.edu\n",
      "{}\n",
      ".stat.uci.edu\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    file_path = 'finalURL.txt' #'/content/urls.txt'\n",
    "    unique_pages, unique_page_count = count_unique_pages(file_path)\n",
    "    print(\"Number of unique pages found:\", unique_page_count)\n",
    "\n",
    "\n",
    "\n",
    "    domainToCheck = ['.ics.uci.edu','cs.uci.edu','informatics.uci.edu','stat.uci.edu']\n",
    "    for domain in domainToCheck:\n",
    "        result = subdomainsCount(domain, file_path)\n",
    "        print(domain)\n",
    "        print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "\n",
    "cluster = ipp.Cluster.from_file(\"/home/ics-home/.ipython/profile_default/security/cluster-.json\")\n",
    "rc = cluster.connect_client_sync()\n",
    "rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
